# Simple Storage Service (S3)

S3 is private by default.

## Ways to setup the access rules

### S3 Bucket Policies

- a form of resource policy
- like identity policy, but attached to a bucket
- resource perspective permissions ("who can access this resource"), hence the `Principal` field
- allow/deny any accounts
- allow/deny anonymous principals

Examples:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicRead",
      "Effect": "Allow",
      "Principal": "*",
      "Action": ["s3:GetObject"],
      "Resource": ["arn:aws:s3:::secretcatproject/*"]
    }
  ]
}
```

```json
{
  "Version": "2012-10-17",
  "Id": "BlockUnLeet",
  "Statement": [
    {
      "Sid": "IPAllow",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:*",
      "Resource": "arn:aws:s3:::secretcatproject/*",
      "Condition": {
        "NotIpAddress": { "aws:SourceIp": "1.3.3.7/32" }
      }
    }
  ]
}
```

There can only be one bucket policy per bucket, but it can contain multiple statements.

### Access Control Lists (ACLs)

ACLs on objects and bucket. ACL is a sub-resource of an object or of a bucket. Legacy. Inflexible & simple permissions.

Permissions which can be granted by ACL:

- `READ`
  - bucket: allows grantee to list the objects in the bucket
  - object: allows grantee to read the object data and its metadata
- `WRITE`
  - bucket: allows grantee to create, overwrite and delete any object in the bucket
  - object: not applicable
- `READ_ACP`
  - bucket: allows grantee to read the bucket ACL
  - object: allows grantee to read the object ACL
- `WRITE_ACP`
  - bucket: allows grantee to write the ACL for the applicable bucket
  - object: allows grantee to write the ACL for the applicable object
- `FULL_CONTROL`
  - bucket: allows grantee the `READ`, `WRITE`, `READ_ACP` and `WRITE_ACP` permissions on the bucket
  - object: allows grantee the `READ`, `READ_ACP` and `WRITE_ACP` permissions on the object

### Block public access

Separate setting to block the public access independently of policies/ACLs.

### How to choose the access control option

- Identity:
  - controlling different resources
  - you have a preference for IAM
  - same account
- Bucket:
  - just controlling S3
  - anonymous or cross-account
- ACLs:
  - never (unless you must)

## Static Website Hosting

Normal access to S3 resources is via AWS APIs. Static website hosting allows access to S3 files via HTTP. Enabling requires setting `Index` and `Error` documents and creates the website endpoint, which is generated using the bucket name. Custom domain can be used, but bucket name still matters in that case. Some good cases for static website hosting are:

- offloading the static traffic from main (dynamic) web-server
- serving the maintenance pages

To use static website hosting, you need to:

- enable static website hosting option for the bucket (and specify `Index` and `Error` pages)
- disable "Block all public access" safeguard
- add the policy with necessary permissions for the bucket

## Pricing

- Storage (per GB/month)
- Data transfer (per GB); "in" is free
- Requests (per 1k operations)

Free tier: 5GB of storage, 20k Get Requests, 2k Put Requests.

## Object versioning

Object versioning is enabled on a bucket level. Initially disabled. Once enabled, cannot be disabled. Can be suspended and re-enabled. So, the states are:

- `disabled` (can be switched to `enabled`)
- `enabled` (can be switched to `suspended`)
- `suspended` (can be switched to `enabled`)

Without versioning enabled, each object identified only by an object key (unique inside the bucket). If you modify an object, the original version is replaced. Versioning allows to store multiple versions of an object within a bucket. Operations, which would modify objects, generate a new version.

S3 objects have an `id` attribute, which is set to `null` when the versioning is disabled. When the versioning is enabled and any object is being modified, S3 generates a new `id` and allocates a new object version with this new `id`, retaining the old version with old `id`. A new object is marked as a latest version (current version).

Deleting an object from the bucket with versioning enabled leads to creation of a **delete marker** â€“ a special type of object version. Deleting the delete marker leads to an object being "restored" to a latest version before the deletion.

Even with versioning enabled you can fully delete a specific object version by specifying the version `id` when removing an object.

All the object's versions consume space.

**MFA Delete** is enabled in versioning configuration. When enabled, an MFA is required to change the bucket versioning state and to delete object's versions. For API calls you need to pass MFA serial number and MFA code.

## S3 Performance Optimization

### Multipart upload

Allows to break the file into chunks, so in case of the failure only the failed chunks will be restarted, and not the whole file. Also speeds up an upload by uploading multiple chunks simultaneously. Minimum file size to enable MU (multipart upload) is 100Mb; Maximum number of chunks is 10k, ranging from 5Mb to 5Gb (last chunk can be smaller than 5Mb).

### Accelerated transfer

Disabled by default for a bucket. Bucket name restrictions to enable: no periods, DNS compatible. When enabled, S3 uploads will use the nearest AWS edge location, which speeds up the uploading process. After enabling the accelerated transfer you will be presented with the special accelerated endpoint for a bucket, and will need to use it to benefit from the acceleration. This will also incur an additional fee.

You can use [this website](https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html) to test the speed increase for the accelerated transfer.

## Key Management Service (KMS)

Regional and public service. Allows to create, store and manage cryptographic keys (symmetric and asymmetric). Able to execute cryptographic operations (encrypt, decrypt, etc.).

Keys never leave KMS. Provides `FIPS 140-2 (L2)`-compliant service. KMS Keys are logical containers for actual cryptographic keys, containing: ID, creation date, policy, description, state; and backed by physical key material (can be generated or imported). KMS Keys can be used to encrypt up to 4KB of data.

Data Encryption Keys (DEKs) generated using KMS Key by `GenerateDataKey` operation. DEKs can encrypt more than 4KB of data.

Common DEK usage flow is:

- generate the DEK using KMS Key
- receive plaintext and encrypted versions of DEK
- encrypt the data using plaintext DEK and discard the plaintext DEK
- store encrypted DEK with encrypted data

KMS Keys are isolated to a region and never leave. There are also a multi-region keys (see later). Keys can be AWS Owned and Customer Owned. Also, customer owned keys can be AWS Managed or Customer Managed. KMS Keys support rotation. With AWS Managed keys rotation is mandatory and cannot be disabled. KMS Key contains backing key (and previous backing keys). You can create aliases for KMS Keys.

Example of KMS Key policy:

```json
{
  "Sid": "Enable IAM User Permissions",
  "Effect": "Allow",
  "Principal": { "AWS": "arn:aws:iam::1234567890:root" },
  "Action": "kms:*",
  "Resource": "*"
}
```

Example of KMS commands to encrypt the `battleplans.txt` file and to decrypt the resulting encrypted file:

```sh
aws kms encrypt \
    --key-id alias/catrobot \
    --plaintext fileb://battleplans.txt \
    --output text \
    --query CiphertextBlob \
    | base64 --decode > not_battleplans.enc

aws kms decrypt \
    --ciphertext-blob fileb://not_battleplans.enc \
    --output text \
    --query Plaintext | base64 --decode > decryptedplans.txt
```

## S3 Server-Side Encryption (SSE)

Buckets aren't encrypted, objects are. Each object inside a bucket might be using different encrypting settings.

By default data in transit is encrypted (HTTPS) between the user/app and the S3 endpoint for both Client-Side Encryption and Server-Side Encryption. The difference is that for Client-Side Encryption the data itself is encrypted on the user/app side, but for Server-Side Encryption the data comes to the S3 Endpoint in plain form (but through HTTPS), and S3 encrypts the data on a server side.

S3 recently made Server-Side Encryption mandatory. Three types of S3 SSE:

- Server-Side Encryption with Customer-Provided Keys (SSE-C)
  - customer manages keys
  - S3 manages encryption
  - the flow:
    - user provides an unencrypted file and a key
    - S3 encrypts the file using the provided key and stores the encrypted file and the hash of the key (key itself is not stored)
- Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) (the default)
  - S3 manages encryption and keys
  - the flow:
    - user provides an unencrypted file
    - S3 generates a unique key per object (uploaded file) and uses that key to encrypt the file (seems to be using AES256)
    - user does't have access to key at any stage
    - S3 encrypts the key itself and stores both encrypted object and encrypted object key
- Server-Side Encryption with KMS keys stored in AWS KMS (SSE-KMS)
  - the same as SSE-S3, but the keys are managed by KMS
  - allows to use customer managed KMS keys (with isolated permissions)
  - you can decrypt the S3 objects only if you have access to a KMS key used to encrypt them (role separation)

## S3 Bucket Keys

When using SSE-KMS, each time the object is being uploaded, S3 makes a KMS call to generate unique DEK and encrypt the object. This might cost a lot when many files uploaded often. Also, there are throttling on KMS calls. To reduce the cost and get around the throttling, KMS can generate the time limited bucket key, which is used to generate DEKs within S3. When using S3 Bucket Keys, it is important to note:

- CloudTrail KMS events now show the bucket instead of objects
- Works with replication: the object encryption is maintained (ETAG changes while replicating the object)

## S3 Storage Classes

### S3 Standard

Object are replicated between at least 3 AZs in the AWS region. 99.999999999% durability; for 10M objects, 1 object loss per 10K years. Replication over 3AZs, content-MD5 checksums, cyclic redundancy checks are used to detect and fix any data corruption.

You are billed a GB/month fee for data stored, a $ per GB charge for transfer OUT (IN is free), price per 1K requests. No specific retrieval fee, no minimum duration, no minimum size.

Should be used for frequently accessed data which is important and non-replaceable.

### S3 Standard-IA (infrequent access)

Most is the same as for `S3 Standard`. Differences:

- storage costs are much cheaper
- per-GB data retrieval fee (cost increases with frequent data access)
- minimum duration charge of 30 days
- minimum capacity charge of 128Kb per object

Should be used for long-lived data, which is important but where access is infrequent.

Do not use if:

- you access the data very often
- you store the data short-term
- you store lots of tiny objects

### S3 One Zone-IA (infrequent access)

Almost the same as `S3 Standard-IA`. Differences:

- cheaper than both `S3 Standard` and `S3 Standard-IA`
- data is stored only in one AZ in the region
- data is replicated within AZ, but if AZ fails, data might be lost

Should be used for long-lived data, which is non-critical and replaceable and where access is infrequent.

### S3 Glacier - Instant

Similar to `S3 Standard-IA`, but:

- cheaper storage
- more expensive retrieval
- longer minimum duration charge (90 days)

Should be used for long-lived data, accessed once per quarter, with millisecond access.

### S3 Glacier - Flexible

Similar to `S3 Glacier - Instant`, but:

- cheaper storage
- objects cannot be made publicly accessible
- access to object's data requires a **retrieval process** (to `S3 Standard-IA`); types of retrieval jobs are (faster is more expensive):
  - expedited (1-5 minutes)
  - standard (3-5 hours)
  - bulk (5-12 hours)
- minimum billable size: 40kb
- minimum billable duration: 90 days

Data is in a "chill state". So, the first byte latency here is minutes or hours.

Should be used for archival data where frequent or realtime access isn't needed.

### S3 Glacier Deep Archive

Similar to `S3 Glacier - Flexible`, but:

- minimum billable duration: 180 days
- retrieval jobs take longer:
  - standard (12 hours)
  - bulk (up to 48 hours)

Data is in a "frozen state". So, the first byte latency here is hours or days.

Should be used for archival data that rarely if ever needs to be accessed (example: legal or regulation data storage).

### S3 Intelligent-Tiering

Object are moved between different storage tiers automatically based on the access patterns. As object is not accessed within X days, it is moved down one tier. When accessed, it is moved back up.

Storage tiers are:

- frequent access (~`S3 Standard`)
- infrequent access (~`S3 Standard-IA`) (X = 30 days)
- archive instant access (~`S3 Glacier - Instant`) (X = 90 days)
- archive access (~`S3 Glacier - Flexible`) (X = 90-270 days) (optional, requires async access via specific API calls)
- deep archive (~`S3 Glacier Deep Archive`) (X = 180-730 days) (optional, requires async access via specific API calls)

There are no retrieval fees upon automatic object transfer between tiers. But there is monitoring and automation cost per 1000 objects.

Should be used for long-lived data with changing or unknown patterns.

## S3 Lifecycle Configuration

Set of rules; each rule consist of actions on a bucket or group of objects (defined by prefix or tags).

Action types:

- transition (between storage classes)
- expiration (removal)

Each storage tier can transition to any of the tiers below it (exceptions are noted); for example, `S3 Standard` can transition to any other tier; `S3 Glacier Deep Archive` cannot transition at all:

- `S3 Standard`
- `S3 Standard-IA`
- `S3 Intelligent-Tiering`
- `S3 One Zone-IA`
  - exception: unable to transition to `S3 Glacier - Instant`
- `S3 Glacier - Instant`
- `S3 Glacier - Flexible`
- `S3 Glacier Deep Archive`

Note that smaller objects can cost more then transitioned. There is minimum of 30 days before transition. A single rule cannot transition to both Glacier and non-Glacier classes.

## S3 Replication

- Cross-Region Replication (CRR)
- Same-Region Replication (SRR)

Replication configuration (setup on a source bucket) specifies:

- destination bucket
- IAM role to use for replication

Replication uses SSL.

S3 Replication Options:

- all objects or a subset
- storage class (default is to maintain)
- ownership (default is the source account; could be problematic when buckets are in different accounts)
- Replication Time Control (RTC): provides some guarantees regarding sync state

By default replication is not retroactive. Versioning needs to be enabled. Batch replication can be used to replicate existing objects. By default replication is one-way: source to destination. Replication is able to handle unencrypted, SSE-S3, SSE-KMS, SSE-C. Source bucket owner needs permissions to objects. No system events (only user events), Glacier or Glacier Deep Archive. By default delete markers are not replicated (can be enabled via `DeleteMarkerReplication`).

Replication usage examples:

- SRR
  - log aggregation
  - PROD and TEST sync
  - resilience with strict sovereignty
- CRR
  - global resilience improvements
  - latency reduction

Replication is enabled via adding replication rules in Bucket Management.

## S3 Presigned URLs

Used to access objects from a non-public bucket via HTTP.

Request to S3 (needs to be initiated by an authenticated AWS principal) to generate a presigned URL requires:

- security credentials
- bucket name
- object key
- expiry date/time
- access type

Tips & quirks:

- You can create a URL for an object you have no access to (and even for a non-existing object).
- When using the URL, the permissions match the identity which generated it.
- Access denied could mean the generating identity never had access or doesn't now.
- Don't generate presigned URLs with a role, because URL stops working when temporary credentials expire.

An example AWS CLI command to generate a presigned S3 URL:

```sh
aws s3 presign s3://bucket/object.ext --expires-in 180
```

## S3 Select and Glacier Select

S3 can store huge objects, up to 5TB. S3/Glacier Select let you use SQL-like statements to retrieve parts of objects. Allow to select on:

- CSV
- JSON
- Parquet
- BZIP2 for CSV and JSON

## S3 Events

Notification generated when events occur in a bucket. Can be delivered to `SNS`, `SQS`, `Lambda` functions. Events generated when:

- Object created (Put/Post/Copy/CompleteMultiPartUpload)
- Object deleted (\*/Delete/DeleteMarkerCreated)
- Object restored (Post/Completed)
- Replication (OperationMissedThreshold/OperationReplicatedAfterThreshold/OperationNotTracked/OperationFailedReplication)

Event Notification Config for S3 Bucket describes rules for these types of events (create/delete/restore/replicate). You also need to create resource policy for each destination used (`SNS` topic / `SQS` queue / `Lambda` function) allowing S3 principal access.

Events are JSON objects, containing event info (bucket/object/etc.).

## S3 Access Logs

Logs are generated for the Source Bucket and stored in the Target Bucket. `S3 Log Delivery Group` reads the logging configuration for the Source Bucket. Best Efforts log delivery: access to Source Bucket is usually logged in Target Bucket within a few hours. You also need to configure Target Bucket ACL to allow `S3 Log Delivery Group` write access. Logs are delivered as log-files with newline-delimited records with space-delimited fields.

## S3 Object Lock

Object Lock is enabled on new buckets (for existing buckets you need to contact support). Versioning also needs to be enabled (individual versions are locked). Write-Once-Read-Many (`WORM`): no delete, no overwrite. Object retention managed via:

- S3 Object Lock: Retention
- S3 Object Lock: Legal Hold
  You can have both, one or the other, or none. A bucket can have default Object Lock settings.

### S3 Object Lock: Retention

Specify **days** and **years** â€“Â a retention period. Two modes of retention period lock:

- **compliance**: can't be adjusted, deleted, overwritten (object versions / retention settings cannot be changed even by the account root user, until the retention period has expired)
- **governance**: special permissions (`s3:BypassGovernanceRetention`) can be granted allowing lock settings to be adjusted (`x-amz-bypass-governance-retention:true` header is also required). Usage:
  - prevent accidental deletion
  - process/governance reasons
  - test of settings before picking compliance mode

### S3 Object Lock: Legal Hold

Set on an object version: **on** or **off**. No retention specified. No deletes or changes until removed. Special permission `s3:PutObjectLegalHold` is required to add or remove the legal hold. Useful to prevent accidental deletion of critical object versions.

## S3 Access Points

Simplify managing access to S3 Buckets/Objects. Rather than having one bucket with one bucket policy you create many access points: each with different policies, each with different network access controls. Each access point has its own endpoint address. You can create an access point with the following AWS CLI command:

```sh
aws s3control create-access-point --name exampleap --account-id 123456789012 --bucket examplebucket
```

Bucket policy needs to include all the permissions of bucket's access points' policies. More information [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/creating-access-points.html#access-points-policies).

## Multi-Region Access Points (MRAP)

You can create a multi-region access point for several buckets from different regions. Creation process can take up to 24hrs.

MRAP have ARN and alias.

You can create replication rules for MRAP. You can either replicate all objects across all buckets or replicate objects from one or more source buckets to one or more destination buckets.

## S3 Quiz

- Which type of S3 encryption shows as AES256
  - SSE-S3
- Which S3 Storage class is suitable for data which is easily replaced (choose the most cost effective)
  - S3 One Zone-IA
- Which Object class in S3 is ideal for uncertain access and low admin overhead
  - S3 Intelligent-Tiering
- What is the cheapest S3 storage class for important data which need to be retained for long periods and is rarely accessed
  - S3 Glacier
- Which steps are required to allow an S3 bucket to operate as a website (choose all which apply)
  - Upload web files
  - Set index and error documents
  - Enable static web hosting
  - Disable block public access settings
  - Add a bucket policy
- What S3 feature allows objects storage classes to be changed and objects deleted automatically
  - S3 Lifecycle policies
- What is the default limit of the number of S3 buckets in an AWS account
  - 100
- How large can an object in S3 be ? and what (if any) limits are there on the number of objects in a bucket
  - Object Max = 5TB, No Object bucket limit
- What S3 feature needs to be enabled to allow Cross-Region Replication (CRR)
  - Versioning
- What S3 feature can be used to grant external accounts access to an S3 bucket
  - Resource Policies
- Which type of encryption allows for role separation where an S3 Full Admin might not be able to decrypt objects
  - SSE-KMS
- Which type of encryption is where AWS perform encryption operations but DON'T hold any keys
  - SSE-C
- What type of encryption means AWS perform the encryption operations and handle key creation & management
  - SSE-S3
- What feature is required to allow CRR to function
  - Versioning
- What happens when an object is deleted in a bucket with versioning enabled
  - A delete marker is added